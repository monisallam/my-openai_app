# Codex AI example application

This is an example that utilizes a lot of the same code from the [name generator app](https://github.com/openai/openai-quickstart-python). It uses the [Flask](https://flask.palletsprojects.com/en/2.0.x/) web framework, which runs in a container, as well as [JupyterLab](https://github.com/jupyter/docker-stacks), which also runs in a container. These containers are built using [Docker](https://docs.docker.com/).

The application itself is very simple. It takes a link to a csv (for the purposes of this app, its best to only give a link to a .csv, one that is accesible via the internet, for example this [soccer data set](https://github.com/fivethirtyeight/data/tree/master/soccer-spi) or [this polling data set](https://github.com/fivethirtyeight/data/tree/master/polls). These are github pages with csv files that can be downloaded. The application will then make a request to the Codex API, which will return python code to read that csv file into a Pandas DataFrame, as well as print out some metadata about the DataFrame. All of this code is generated by the API itself. From there, the application will convert it to a Jupyter notebook, and place it inside the Jupyter container so that it is ready to run. The goal is to provide a data science "jump start" environment with minimal coding necessary - you can just log into the Jupyter console and start manipulating data!

An improvement to this would be to add in the ability to read more than just csv files, as well as files that are not publicly acessible (a kaggle data set for example), where an API key may be required. The ability to read more than just csv files would require some more logic inside the application, as well as a different prompt inside the Codex API call. Also, adding more functionality to the app itself beyond being a single page would be a definite next step.

## Setup

1. If you donâ€™t have Python installed, [install it from here](https://www.python.org/downloads/)

2. If you don't have Docker installed, [install it from here](https://docs.docker.com/engine/install/). Make sure that the docker-compose-plugin is also installed. This is in the instructions for Docker Engine, which is the preferred way to install for this application and the following instructions. Using Docker Desktop may laed to unexpected behavior.

3. Clone this repository

4. Navigate into the project directory

   ```bash
   $ cd my-openai_app
   ```
5. Make a copy of the example environment variables file

   ```bash
   $ cp .env.example .env
   ```
   
6. Add your [API key](https://beta.openai.com/account/api-keys) to the newly created `.env` file. Make sure you have access to the Codex API Beta.

7. Because we are using containers, a python virtualenv is not necessary to run the application, as the container will have its own version of python as well its own requirements file. Using docker compose will build the application container from scratch locally. This may take some time on the initial app start up, but subsequent application invocations should start much quicker, as the container will be building from the local cache. You can also modify the docker-compose.yaml file to pull from the built image, rather than have it build each time. A second container will be pulled from dockerhub, this is the JupyterLab container. This may also take some time on the first invocation, but will be very fast in the subsequent invovations as it will be stored in the local cache.

   ```bash
   $ docker compose up
   ```
This will start the application container, as well as the notebook container. The application can be reached at http://localhost:5000. Navigate to the url on your machine and you should see the application. Copy and paste a csv file url. Once python code is returned, you can keep feeding the application new files, and it will make API calls to the Codex API, and convert what is returned to a new Jupyter notebook file, contained in the Jupyter Lab container, ready to use. 

**IMPORTANT - There will be output that is important. The JupyterLab container will require a token for authentication. This will look like**

```bash
$ http://f8cd4b5f5d4a:8888/lab?token=15a691eadb6749ce978be9aacfb524e2fe9aab782df34044
```

Your token value will be different. It is different each time a NEW container is built. If you use the same JupyterLab container for subsequent invocations, this will be a one time thing. You will need to navigate to ```http://localhost:10000/lab?token=<your-token-value>```.

8. Log in to the JupyterLab container at ```http://localhost:10000/lab?token=<your-token-value>```. Each csv file that you supply the app will have a notebook contained in the ```work``` directory, which you can view in the JupyterLab console. The code generated will be different for each csv file, but it the Codex API is extremely impressive at being able to intelligently fetch, and make data readily usable.

9. All of the notebooks that are generated by the application will persist once the containers exit, in the ```output_notebooks``` directory on the host, as well as the JupyterLab container. 

Enjoy!
 
